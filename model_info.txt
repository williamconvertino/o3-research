1. The Core Idea: Viewing the Forward Pass as In-Context Gradient Descent
Latent Function & Problem Setup:
We begin with a categorical regression formulation where we have data pairs 
(
ğ‘¥
ğ‘–
,
ğ‘¦
ğ‘–
)
(x 
i
â€‹
 ,y 
i
â€‹
 ) for 
ğ‘–
=
1
,
â€¦
,
ğ‘
i=1,â€¦,N. Here:

ğ‘¥
ğ‘–
x 
i
â€‹
  are the covariates (learned positional encodings), treated as fixed during a forward pass.
ğ‘¦
ğ‘–
y 
i
â€‹
  are the target tokens.
The latent function is defined as:

ğ‘“
(
ğ‘¥
)
=
ğ´
ğ‘¥
,
f(x)=Ax,
where 
ğ´
A is the weight matrix that we would typically update using gradient descent to maximize the likelihood of the observed tokens via a softmax classifier. In particular, we set the probability for class 
ğ‘
c as:

ğ‘
(
ğ‘Œ
=
ğ‘
âˆ£
ğ‘“
(
ğ‘¥
)
)
=
exp
â¡
(
ğ‘Š
ğ‘
â€‰
ğ‘“
(
ğ‘¥
)
)
âˆ‘
ğ‘
â€²
exp
â¡
(
ğ‘Š
ğ‘
â€²
â€‰
ğ‘“
(
ğ‘¥
)
)
,
p(Y=câˆ£f(x))= 
âˆ‘ 
c 
â€²
 
â€‹
 exp(W 
c 
â€²
 
â€‹
 f(x))
exp(W 
c
â€‹
 f(x))
â€‹
 ,
where each 
ğ‘Š
ğ‘
W 
c
â€‹
  is a token embedding (fixed within a forward pass).

Gradient Descent Update:
The loss is given by:

ğ¿
(
ğ´
)
=
1
ğ‘
âˆ‘
ğ‘–
=
1
ğ‘
log
â¡
ğ‘
(
ğ‘Œ
=
ğ‘¦
ğ‘–
âˆ£
ğ‘“
(
ğ‘¥
ğ‘–
)
)
.
L(A)= 
N
1
â€‹
  
i=1
âˆ‘
N
â€‹
 logp(Y=y 
i
â€‹
 âˆ£f(x 
i
â€‹
 )).
Differentiating the loss with respect to 
ğ´
A (with 
ğ‘“
(
ğ‘¥
ğ‘–
)
=
ğ´
ğ‘¥
ğ‘–
f(x 
i
â€‹
 )=Ax 
i
â€‹
 ) gives us:

âˆ‚
ğ¿
âˆ‚
ğ´
=
1
ğ‘
âˆ‘
ğ‘–
=
1
ğ‘
[
ğ‘Š
ğ‘¦
ğ‘–
âˆ’
ğ¸
ğ‘
âˆ¼
ğ‘
(
ğ‘
âˆ£
ğ‘“
(
ğ‘¥
ğ‘–
)
)
[
ğ‘Š
ğ‘
]
]
ğ‘¥
ğ‘–
âŠ¤
.
âˆ‚A
âˆ‚L
â€‹
 = 
N
1
â€‹
  
i=1
âˆ‘
N
â€‹
 [W 
y 
i
â€‹
 
â€‹
 âˆ’E 
câˆ¼p(câˆ£f(x 
i
â€‹
 ))
â€‹
 [W 
c
â€‹
 ]]x 
i
âŠ¤
â€‹
 .
With a gradient step 
ğ´
â†’
ğ´
+
ğ›¼
â€‰
âˆ‚
ğ¿
âˆ‚
ğ´
Aâ†’A+Î± 
âˆ‚A
âˆ‚L
â€‹
 , this corresponds to updating 
ğ´
A by adding:

Î”
ğ´
â€‰
ğ‘¥
ğ‘—
=
ğ›¼
â€‰
1
ğ‘
âˆ‘
ğ‘–
=
1
ğ‘
[
ğ‘Š
ğ‘¦
ğ‘–
âˆ’
ğ¸
ğ‘
âˆ¼
ğ‘
(
ğ‘
âˆ£
ğ‘“
(
ğ‘¥
ğ‘–
)
)
[
ğ‘Š
ğ‘
]
]
(
ğ‘¥
ğ‘–
âŠ¤
ğ‘¥
ğ‘—
)
.
Î”Ax 
j
â€‹
 =Î± 
N
1
â€‹
  
i=1
âˆ‘
N
â€‹
 [W 
y 
i
â€‹
 
â€‹
 âˆ’E 
câˆ¼p(câˆ£f(x 
i
â€‹
 ))
â€‹
 [W 
c
â€‹
 ]](x 
i
âŠ¤
â€‹
 x 
j
â€‹
 ).
The inner product 
ğ‘¥
ğ‘–
âŠ¤
ğ‘¥
ğ‘—
x 
i
âŠ¤
â€‹
 x 
j
â€‹
  acts as a kernel measuring the similarity between positions.

Manifesting the Update in the Function Output:
Instead of explicitly updating a parameter matrix 
ğ´
A, we â€œmanifestâ€ the gradient descent update in the function output:

Start with 
ğ´
0
=
0
A 
0
â€‹
 =0 so that 
ğ‘“
0
(
ğ‘¥
)
=
0
f 
0
â€‹
 (x)=0.
Compute the first update:
ğ‘“
1
(
ğ‘¥
ğ‘—
)
=
Î”
ğ´
0
â€‰
ğ‘¥
ğ‘—
=
ğ›¼
â€‰
1
ğ‘
âˆ‘
ğ‘–
=
1
ğ‘
[
ğ‘Š
ğ‘¦
ğ‘–
âˆ’
ğ¸
ğ‘
[
ğ‘Š
ğ‘
]
ğ‘“
0
(
ğ‘¥
ğ‘–
)
]
(
ğ‘¥
ğ‘–
âŠ¤
ğ‘¥
ğ‘—
)
.
f 
1
â€‹
 (x 
j
â€‹
 )=Î”A 
0
â€‹
 x 
j
â€‹
 =Î± 
N
1
â€‹
  
i=1
âˆ‘
N
â€‹
 [W 
y 
i
â€‹
 
â€‹
 âˆ’E 
c
â€‹
 [W 
c
â€‹
 ] 
f 
0
â€‹
 (x 
i
â€‹
 )
â€‹
 ](x 
i
âŠ¤
â€‹
 x 
j
â€‹
 ).
For subsequent layers, the output is accumulated:
ğ‘“
ğ‘˜
+
1
(
ğ‘¥
ğ‘—
)
=
ğ‘“
ğ‘˜
(
ğ‘¥
ğ‘—
)
+
Î”
ğ´
ğ‘˜
â€‰
ğ‘¥
ğ‘—
,
f 
k+1
â€‹
 (x 
j
â€‹
 )=f 
k
â€‹
 (x 
j
â€‹
 )+Î”A 
k
â€‹
 x 
j
â€‹
 ,
so that after 
ğ¾
K layers,
ğ‘“
ğ¾
(
ğ‘¥
ğ‘—
)
=
âˆ‘
ğ‘˜
=
0
ğ¾
âˆ’
1
Î”
ğ´
ğ‘˜
â€‰
ğ‘¥
ğ‘—
.
f 
K
â€‹
 (x 
j
â€‹
 )= 
k=0
âˆ‘
Kâˆ’1
â€‹
 Î”A 
k
â€‹
 x 
j
â€‹
 .
This process effectively â€œperformsâ€ gradient descent over the forward pass without maintaining an explicit 
ğ´
A at every step.

2. Connection to the Transformer Architecture
Attention Mechanism Analogy:

The similarity measure 
ğ‘¥
ğ‘–
âŠ¤
ğ‘¥
ğ‘—
x 
i
âŠ¤
â€‹
 x 
j
â€‹
  is analogous to the dot product of queries and keys in attention.
The term 
[
ğ‘Š
ğ‘¦
ğ‘–
âˆ’
ğ¸
ğ‘
[
ğ‘Š
ğ‘
]
ğ‘“
(
ğ‘¥
ğ‘–
)
]
[W 
y 
i
â€‹
 
â€‹
 âˆ’E 
c
â€‹
 [W 
c
â€‹
 ] 
f(x 
i
â€‹
 )
â€‹
 ] serves as an error signal, which in this formulation takes the role of the â€œvalueâ€ vector.
The constants 
ğ›¼
Î± and 
1
/
ğ‘
1/N can be absorbed into a final projection matrix 
ğ‘Š
ğ‘œ
W 
o
â€‹
 , paralleling how transformers use a learned projection after attention.
Generalizing with Kernels:
Instead of the linear inner product, we can generalize:

ğ‘“
(
ğ‘¥
)
=
ğ´
â€‰
ğœ“
(
ğ‘¥
)
,
f(x)=AÏˆ(x),
with a kernel defined as:

ğ‘˜
(
ğ‘¥
ğ‘–
,
ğ‘¥
ğ‘—
)
=
ğœ“
(
ğ‘¥
ğ‘–
)
âŠ¤
ğœ“
(
ğ‘¥
ğ‘—
)
.
k(x 
i
â€‹
 ,x 
j
â€‹
 )=Ïˆ(x 
i
â€‹
 ) 
âŠ¤
 Ïˆ(x 
j
â€‹
 ).
This opens up the possibility of using Mercer kernels (or approximations thereof) or even the softmax kernel as in standard transformers. The softmax can be seen as a non-symmetric alternative that relaxes the requirement of symmetry in a Mercer kernel.

Multiple Heads:
We extend the model to multiple heads:

ğ‘“
(
ğ‘¥
)
=
âˆ‘
â„
=
1
ğ»
ğ´
â„
â€‰
ğœ“
â„
(
ğ‘¥
)
,
f(x)= 
h=1
âˆ‘
H
â€‹
 A 
h
â€‹
 Ïˆ 
h
â€‹
 (x),
where each head uses its own transformation (via matrices analogous to 
ğ‘Š
ğ‘
W 
q
â€‹
  and 
ğ‘Š
ğ‘˜
W 
k
â€‹
 ) to compute a kernel:

ğ‘˜
â„
(
ğ‘¥
ğ‘–
,
ğ‘¥
ğ‘—
)
=
ğ‘˜
(
ğ‘Š
â„
ğ‘¥
ğ‘–
,
ğ‘Š
â„
ğ‘¥
ğ‘—
)
.
k 
h
â€‹
 (x 
i
â€‹
 ,x 
j
â€‹
 )=k(W 
h
â€‹
 x 
i
â€‹
 ,W 
h
â€‹
 x 
j
â€‹
 ).
Each head computes its own update 
Î”
ğ´
â„
â€‰
ğ‘¥
ğ‘—
Î”A 
h
â€‹
 x 
j
â€‹
 , and these are then aggregated (for example, summed or linearly projected by 
ğ‘Š
ğ‘œ
W 
o
â€‹
 )â€”mirroring multi-head attention.

Skip Connections & Layer Accumulation:
To accumulate updates across layers (each representing a gradient descent step), we use skip connections:

The initial skip is zero since 
ğ‘“
0
(
ğ‘¥
)
=
0
f 
0
â€‹
 (x)=0.
Each subsequent layer adds its delta update:
ğ‘“
ğ‘˜
+
1
(
ğ‘¥
)
=
ğ‘“
ğ‘˜
(
ğ‘¥
)
+
Î”
ğ´
ğ‘˜
â€‰
ğ‘¥
.
f 
k+1
â€‹
 (x)=f 
k
â€‹
 (x)+Î”A 
k
â€‹
 x.
This is analogous to the residual connections in transformers, ensuring that each layerâ€™s contribution is preserved in the final output.

